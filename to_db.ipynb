{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9490e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee5b38e",
   "metadata": {},
   "source": [
    "##TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1734f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "anime_dataset = pd.read_csv('csv/anime-dataset-2023.csv',sep=',')\n",
    "types = list(anime_dataset['Type'].unique())\n",
    "types = sorted(types)\n",
    "text = '--migrate:up \\n\\n'\n",
    "id = 1\n",
    "diccionario_types = {}\n",
    "for n in types:\n",
    "    if n != 'UNKNOWN':\n",
    "        text = text + f\"INSERT INTO types (id, name) VALUES ({id},'{n}');\\n\"\n",
    "        diccionario_types[n] = id\n",
    "        id = id+1\n",
    "text = text + '\\n-- migrate:down \\n\\nDELETE FROM types;'\n",
    "with open('inserts_types.sql','w',encoding=\"utf-8\") as archivo:\n",
    "    archivo.write(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b08c88",
   "metadata": {},
   "source": [
    "##CLASIFICATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a074925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "anime_dataset = pd.read_csv('csv/anime-dataset-2023.csv',sep=',')\n",
    "clasifications = list(anime_dataset['Rating'].unique())\n",
    "clasifications = sorted(clasifications)\n",
    "text = '--migrate:up \\n\\n'\n",
    "id = 1\n",
    "diccionario_clasifications = {}\n",
    "for n in clasifications:\n",
    "    if n != 'UNKNOWN':\n",
    "        text = text + f\"INSERT INTO clasifications (id, name) VALUES ({id},'{n}');\\n\"\n",
    "        diccionario_clasifications[n] = id\n",
    "        id = id+1\n",
    "text = text + '\\n-- migrate:down \\n\\nDELETE FROM clasifications;'\n",
    "with open('inserts_clasifications.sql','w',encoding=\"utf-8\") as archivo:\n",
    "    archivo.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abb797c",
   "metadata": {},
   "source": [
    "##SEASONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f3694ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "anime_dataset = pd.read_csv('csv/anime-dataset-2023.csv',sep=',')\n",
    "seasons_raw = list(anime_dataset['Premiered'].unique())\n",
    "seasons = list(set([s.split()[0] for s in seasons_raw]))\n",
    "text = '--migrate:up \\n\\n'\n",
    "id = 1\n",
    "diccionario_seasons = {}\n",
    "for n in seasons:\n",
    "    if n != 'UNKNOWN':\n",
    "        text = text + f\"INSERT INTO seasons (id, name) VALUES ({id},'{n}');\\n\"\n",
    "        diccionario_seasons[n] = id\n",
    "        id = id+1\n",
    "text = text + '\\n-- migrate:down \\n\\nDELETE FROM seasons;'\n",
    "with open('inserts_seasons.sql','w',encoding=\"utf-8\") as archivo:\n",
    "    archivo.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f61666e",
   "metadata": {},
   "source": [
    "##STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e948e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "anime_dataset = pd.read_csv('csv/anime-dataset-2023.csv',sep=',')\n",
    "statuses = list(anime_dataset['Status'].unique())\n",
    "statuses = sorted(statuses)\n",
    "text = '--migrate:up \\n\\n'\n",
    "id = 1\n",
    "diccionario_statuses = {}\n",
    "for n in statuses:\n",
    "    text = text + f\"INSERT INTO status (id, name) VALUES ({id},'{n}');\\n\"\n",
    "    diccionario_statuses[n] = id\n",
    "    id = id+1\n",
    "text = text + '\\n-- migrate:down \\n\\nDELETE FROM status;'\n",
    "with open('inserts_status.sql','w',encoding=\"utf-8\") as archivo:\n",
    "    archivo.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f121b06",
   "metadata": {},
   "source": [
    "##SOURCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a708b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "anime_dataset = pd.read_csv('csv/anime-dataset-2023.csv',sep=',')\n",
    "sources = list(anime_dataset['Source'].unique())\n",
    "sources = sorted(sources)\n",
    "text = '--migrate:up \\n\\n'\n",
    "id = 1\n",
    "diccionario_sources = {}\n",
    "for n in sources:\n",
    "    if n != 'Unknown':\n",
    "        text = text + f\"INSERT INTO sources (id, name) VALUES ({id},'{n}');\\n\"\n",
    "        diccionario_sources[n] = id\n",
    "        id = id+1\n",
    "text = text + '\\n-- migrate:down \\n\\nDELETE FROM sources;'\n",
    "with open('inserts_sources.sql','w',encoding=\"utf-8\") as archivo:\n",
    "    archivo.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5c1c2f",
   "metadata": {},
   "source": [
    "##NATIONS\n",
    "usando 'all_players.csv' para el listado de nacionalidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b02b32f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MARIEL\\AppData\\Local\\Temp\\ipykernel_9780\\3731192810.py:1: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ds_players = pd.read_csv('csv/all_players.csv', sep=',')\n"
     ]
    }
   ],
   "source": [
    "ds_players = pd.read_csv('csv/all_players.csv', sep=',')\n",
    "\n",
    "nations = list(ds_players['Nation'].unique())\n",
    "nations = sorted(nations)\n",
    "text = '--migrate:up \\n\\n'\n",
    "id = 1\n",
    "diccionario_nations = {}\n",
    "for n in nations:\n",
    "  text = text + f\"INSERT INTO nations (id, name) VALUES ({id}, '{n.replace(\"'\", \"''\")}');\\n\"\n",
    "  diccionario_nations[n] = id\n",
    "  id = id + 1\n",
    "  \n",
    "text = text + '\\n-- migrate:down \\n\\nDELETE FROM nations;'\n",
    "with open('inserts_nations.sql', 'w', encoding=\"utf-8\") as archivo:\n",
    "  archivo.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ba3a8",
   "metadata": {},
   "source": [
    "##GENRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab9acbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "anime_dataset = pd.read_csv('csv/anime-dataset-2023.csv',sep=',')\n",
    "genres_raw = list(anime_dataset['Genres'].unique())\n",
    "genres = []\n",
    "for g in genres_raw:\n",
    "    for genre in g.split(','):\n",
    "        genre = genre.strip()\n",
    "        if genre not in genres:\n",
    "            genres.append(genre)\n",
    "genres = sorted(genres)\n",
    "text = '--migrate:up \\n\\n'\n",
    "id = 1\n",
    "diccionario_genres = {}\n",
    "for n in genres:\n",
    "    if n != 'UNKNOWN':\n",
    "        text = text + f\"INSERT INTO genres (id, name) VALUES ({id},'{n.replace(\"'\", \"''\")}');\\n\"\n",
    "        diccionario_genres[n] = id\n",
    "        id = id+1\n",
    "text = text + '\\n-- migrate:down \\n\\nDELETE FROM genres;'\n",
    "with open('inserts_genres.sql','w',encoding=\"utf-8\") as archivo:\n",
    "    archivo.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803e5683",
   "metadata": {},
   "source": [
    "##PRODUCERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97cf4647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "anime_dataset = pd.read_csv('csv/anime-dataset-2023.csv',sep=',')\n",
    "producers_raw = list(anime_dataset['Producers'].unique())\n",
    "producers = []\n",
    "for g in producers_raw:\n",
    "    for producer in g.split(','):\n",
    "        producer = producer.strip()\n",
    "        if producer not in producers:\n",
    "            producers.append(producer)\n",
    "producers = sorted(producers, key=lambda x: x.lower())\n",
    "text = '--migrate:up \\n\\n'\n",
    "id = 1\n",
    "diccionario_producers = {}\n",
    "for n in producers:\n",
    "    text = text + f\"INSERT INTO producers (id, name) VALUES ({id},'{n.replace(\"'\", \"''\")}');\\n\"\n",
    "    diccionario_producers[n] = id\n",
    "    id = id+1\n",
    "text = text + '\\n-- migrate:down \\n\\nDELETE FROM producers;'\n",
    "with open('inserts_producers.sql','w',encoding=\"utf-8\") as archivo:\n",
    "    archivo.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9737ee",
   "metadata": {},
   "source": [
    "##LICENSORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "549d52df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "anime_dataset = pd.read_csv('csv/anime-dataset-2023.csv',sep=',')\n",
    "licensors_raw = list(anime_dataset['Licensors'].unique())\n",
    "licensors = []\n",
    "for g in licensors_raw:\n",
    "    for licensor in g.split(','):\n",
    "        licensor = licensor.strip()\n",
    "        if licensor not in licensors:\n",
    "            licensors.append(licensor)\n",
    "licensors = sorted(licensors, key=lambda x: x.lower())\n",
    "text = '--migrate:up \\n\\n'\n",
    "id = 1\n",
    "diccionario_licensors = {}\n",
    "for n in licensors:\n",
    "    text = text + f\"INSERT INTO licensors (id, name) VALUES ({id},'{n.replace(\"'\", \"''\")}');\\n\"\n",
    "    diccionario_licensors[n] = id\n",
    "    id = id+1\n",
    "text = text + '\\n-- migrate:down \\n\\nDELETE FROM licensors;'\n",
    "with open('inserts_licensors.sql','w',encoding=\"utf-8\") as archivo:\n",
    "    archivo.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b596199b",
   "metadata": {},
   "source": [
    "##STUDIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f7d652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "anime_dataset = pd.read_csv('csv/anime-dataset-2023.csv',sep=',')\n",
    "studios_raw = list(anime_dataset['Studios'].unique())\n",
    "studios = []\n",
    "for g in studios_raw:\n",
    "    for studio in g.split(','):\n",
    "        studio = studio.strip()\n",
    "        if studio not in studios:\n",
    "            studios.append(studio)\n",
    "studios = sorted(studios, key=lambda x: x.lower())\n",
    "text = '--migrate:up \\n\\n'\n",
    "id = 1\n",
    "diccionario_studios = {}\n",
    "for n in studios:\n",
    "    text = text + f\"INSERT INTO studios (id, name) VALUES ({id},'{n.replace(\"'\", \"''\")}');\\n\"\n",
    "    diccionario_studios[n] = id\n",
    "    id = id+1\n",
    "text = text + '\\n-- migrate:down \\n\\nDELETE FROM studios;'\n",
    "with open('inserts_studios.sql','w',encoding=\"utf-8\") as archivo:\n",
    "    archivo.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e088a54",
   "metadata": {},
   "source": [
    "##GENDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26515388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "user_dataset = pd.read_csv('csv/users-details-2023.csv',sep=',')\n",
    "genders_raw = list(user_dataset['Gender'].dropna().unique())\n",
    "genders = []\n",
    "for g in genders_raw:\n",
    "    for gender in g.split(','):\n",
    "        gender = gender.strip()\n",
    "        if gender not in genders:\n",
    "            genders.append(gender)\n",
    "genders = sorted(genders)\n",
    "text = '--migrate:up \\n\\n'\n",
    "id = 1\n",
    "diccionario_genders = {}\n",
    "for n in genders:\n",
    "    text = text + f\"INSERT INTO genders (id, name) VALUES ({id},'{n}');\\n\"\n",
    "    diccionario_genders[n] = id\n",
    "    id = id+1\n",
    "text = text + '\\n-- migrate:down \\n\\nDELETE FROM genders;'\n",
    "with open('inserts_genders.sql','w',encoding=\"utf-8\") as archivo:\n",
    "    archivo.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee414b",
   "metadata": {},
   "source": [
    "##USERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "135d0113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "user_dataset = pd.read_csv('csv/users-details-2023.csv',sep=',')\n",
    "users = user_dataset[['Username', 'Birthday', 'Joined', 'Total Entries', 'Mean Score','Gender']].head(1000)\n",
    "text = '-- migrate:up \\n\\n'\n",
    "diccionario_users = {}\n",
    "id = 1\n",
    "for index, row in users.iterrows():\n",
    "    username = row['Username']\n",
    "    birthday = str(row['Birthday'])[:10] if pd.notna(row['Birthday']) else None\n",
    "    joined = str(row['Joined'])[:10] if pd.notna(row['Joined']) else None\n",
    "    mean_score = row['Mean Score'] if pd.notna(row['Mean Score']) else 0\n",
    "    total_entries = int(row['Total Entries']) if pd.notna(row['Total Entries']) else 0\n",
    "    gender_id = random.randint(1,3)\n",
    "    nation_id = random.randint(1,154)\n",
    "\n",
    "    birthday_sql = f\"TO_DATE('{birthday}', 'YYYY-MM-DD')\" if birthday else 'NULL'\n",
    "    joined_sql = f\"TO_DATE('{joined}', 'YYYY-MM-DD')\" if joined else 'NULL'\n",
    "\n",
    "    text += (\n",
    "        f\"INSERT INTO users (id, username, birthday, joined, total_entries, mean_score, gender_id, nation_id) \"\n",
    "        f\"VALUES ({id}, '{username}', {birthday_sql}, {joined_sql}, {total_entries}, {mean_score}, {gender_id}, {nation_id});\\n\"\n",
    "    )\n",
    "    \n",
    "    diccionario_users[id] = id\n",
    "    id = id + 1\n",
    "\n",
    "text += '\\n-- migrate:down\\n\\nDELETE FROM users;'\n",
    "\n",
    "# Guardar en archivo .sql\n",
    "with open('inserts_users.sql','w',encoding =\"utf-8\") as archivo:\n",
    "    archivo.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f390c375",
   "metadata": {},
   "source": [
    "##ANIMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5edf4e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "anime_dataset = pd.read_csv('csv/anime-dataset-2023.csv',sep=',')\n",
    "\n",
    "animes = anime_dataset[['anime_id', 'Name', 'English name', 'Other name', 'Score', 'Synopsis', 'Type',\n",
    "                        'Episodes', 'Aired', 'Premiered', 'Status', 'Source', 'Duration', 'Rating',\n",
    "                        'Rank', 'Popularity', 'Favorites', 'Scored By', 'Members', 'Image URL']].head(10000)\n",
    "\n",
    "\n",
    "def escapar(texto):\n",
    "    return str(texto).replace(\"'\", \"''\").strip()\n",
    "\n",
    "def es_unicode(texto):\n",
    "    return re.search(r'[^\\x00-\\x7F]', texto) is not None\n",
    "\n",
    "text = '-- migrate:up \\n\\n'\n",
    "id = 1\n",
    "\n",
    "diccionario_animes = {}\n",
    "for _, row in animes.iterrows():\n",
    "    name = escapar(row['Name']) if pd.notna(row['Name']) else 'Desconocido'\n",
    "    english_name = f\"'{escapar(row['English name'])}'\" if pd.notna(row['English name']) else 'NULL'\n",
    "    japanese_raw = escapar(row['Other name']) if pd.notna(row['Other name']) else ''\n",
    "    japanese_name = f\"N'{japanese_raw}'\" if es_unicode(japanese_raw) else f\"'{japanese_raw}'\" if japanese_raw else 'NULL'\n",
    "    synopsis = f\"'{escapar(row['Synopsis'])}'\" if pd.notna(row['Synopsis']) else \"'Sin descripci√≥n'\"\n",
    "\n",
    "    try: episodes = int(float(row['Episodes']))\n",
    "    except: episodes = 'NULL'\n",
    "    try: scored_by = int(float(row['Scored By']))\n",
    "    except: scored_by = 0\n",
    "    try: favorites = int(float(row['Favorites']))\n",
    "    except: favorites = 0\n",
    "    try: members = int(float(row['Members']))\n",
    "    except: members = 0\n",
    "    try: popularity = int(float(row['Popularity']))\n",
    "    except: popularity = 0\n",
    "    try: rank = int(float(row['Rank']))\n",
    "    except: rank = 0\n",
    "    try: score = float(row['Score'])\n",
    "    except: score = 0.0\n",
    "\n",
    "    duration_raw = str(row['Duration']).lower()\n",
    "    duration = 0\n",
    "    if 'hr' in duration_raw:\n",
    "        try:\n",
    "            h = int(duration_raw.split('hr')[0].strip())\n",
    "            duration += h * 60\n",
    "            duration_raw = duration_raw.split('hr')[1]\n",
    "        except: pass\n",
    "    if 'min' in duration_raw:\n",
    "        try:\n",
    "            m = int(duration_raw.split('min')[0].strip())\n",
    "            duration += m\n",
    "        except: pass\n",
    "\n",
    "    image_url = f\"'{escapar(row['Image URL'])}'\"\n",
    "\n",
    "    type_id = diccionario_types.get(row['Type'], random.randint(1, 6))\n",
    "    clasification_id = diccionario_clasifications.get(row['Rating'], random.randint(1, 6))\n",
    "    status_id = diccionario_statuses.get(row['Status'], random.randint(1, 3))\n",
    "    season_id = diccionario_seasons.get(str(row['Premiered']).split()[0], random.randint(1, 4))\n",
    "    source_id = diccionario_sources.get(row['Source'], random.randint(1, 16))\n",
    "\n",
    "    text += f\"\"\"INSERT INTO animes (\n",
    "    id, name, english_name, other_name, synopsis, episodes, scored_by, members,\n",
    "    favorites, popularity, duration, score, image_url, rank, type_id,\n",
    "    clasification_id, status_id, season_id, source_id\n",
    ") VALUES (\n",
    "    {id},\n",
    "    '{name}',\n",
    "    {english_name},\n",
    "    {japanese_name},\n",
    "    {synopsis},\n",
    "    {episodes},\n",
    "    {scored_by},\n",
    "    {members},\n",
    "    {favorites},\n",
    "    {popularity},\n",
    "    {duration},\n",
    "    {score},\n",
    "    {image_url},\n",
    "    {rank},\n",
    "    {type_id},\n",
    "    {clasification_id},\n",
    "    {status_id},\n",
    "    {season_id},\n",
    "    {source_id}\n",
    ");\\n\\n\"\"\"\n",
    "    diccionario_animes[id] = id\n",
    "    id += 1\n",
    "\n",
    "with open('inserts_animes.sql', 'w', encoding='utf-8') as archivo:\n",
    "    archivo.write(text)\n",
    "\n",
    "\n",
    "\n",
    "with open('inserts_animes.sql','w',encoding =\"utf-8\") as archivo:\n",
    "    archivo.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278a29f0",
   "metadata": {},
   "source": [
    "##ANIMES-GENRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1da8d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "anime_dataset = pd.read_csv('csv/anime-dataset-2023.csv',sep=',')\n",
    "animes = anime_dataset[['anime_id','Genres']].head(10000)\n",
    "text = '-- migrate:up \\n\\n'\n",
    "id = 1\n",
    "for index, row in animes.iterrows():\n",
    "    anime_id = random.randint(1,10000)\n",
    "    if pd.isna(row['Genres']):\n",
    "        continue\n",
    "    genres = [g.strip() for g in row['Genres'].split(',')]\n",
    "    for g in genres:\n",
    "        genre_id = diccionario_genres.get(g, random.randint(1,21))\n",
    "        text += f\"INSERT INTO animes_genres (id, anime_id, genre_id) VALUES ({id}, {anime_id}, {genre_id});\\n\"\n",
    "        id += 1\n",
    "\n",
    "with open('inserts_animes_genres.sql','w',encoding =\"utf-8\") as archivo:\n",
    "    archivo.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa6c142",
   "metadata": {},
   "source": [
    "##RECOMMENDATIONS\n",
    "manualmente xq no hay datos de eso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed9de9",
   "metadata": {},
   "source": [
    "##REVIEWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e0bd76",
   "metadata": {},
   "source": [
    "datos fabricados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e931180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "anime_dataset = pd.read_csv('csv/users-score-2023.csv',sep=',')\n",
    "reviews = anime_dataset[['user_id', 'anime_id']].head(5000)\n",
    "text = '-- migrate:up \\n\\n'\n",
    "id = 1\n",
    "for index, row in reviews.iterrows():\n",
    "    user_id = random.randint(1,1000)\n",
    "    anime_id = random.randint(1,10000)\n",
    "    recommendation_id = random.randint(1,3)\n",
    "    text += f\"INSERT INTO reviews (id, comments, anime_id, user_id, recommendation_id) VALUES ({id}, 'comentario.txt', {anime_id}, {user_id}, {recommendation_id});\\n\"\n",
    "    id += 1\n",
    "\n",
    "with open('inserts_reviews.sql','w',encoding =\"utf-8\") as archivo:\n",
    "    archivo.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ee0ee7",
   "metadata": {},
   "source": [
    "##WATCH STATUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5258ab",
   "metadata": {},
   "source": [
    "a mano xq no hay datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec29f81",
   "metadata": {},
   "source": [
    "##LISTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59857a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "anime_dataset = pd.read_csv('csv/users-score-2023.csv',sep=',')\n",
    "lists = anime_dataset[['user_id', 'anime_id', 'rating']].head(5000)\n",
    "text = '-- migrate:up \\n\\n'\n",
    "id = 1\n",
    "for index, row in lists.iterrows():\n",
    "    rating = row['rating']\n",
    "    rewatched = random.randint(1,50)\n",
    "    user_id = random.randint(1,1000)\n",
    "    anime_id = random.randint(1,10000)\n",
    "    watch_status_id = random.randint(1,5)\n",
    "    text += f\"INSERT INTO lists (id, rating, rewatched, user_id, anime_id, watch_status_id) VALUES ({id}, {rating}, {rewatched}, {user_id}, {anime_id}, {watch_status_id});\\n\"\n",
    "    id += 1\n",
    "\n",
    "with open('inserts_lists.sql','w',encoding =\"utf-8\") as archivo:\n",
    "    archivo.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ae1944",
   "metadata": {},
   "source": [
    "##ANIMES PRODUCERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f524972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "anime_dataset = pd.read_csv('csv/anime-dataset-2023.csv',sep=',')\n",
    "animes = anime_dataset[['anime_id','Producers']].head(5000)\n",
    "text = '-- migrate:up \\n\\n'\n",
    "id = 1\n",
    "for index, row in animes.iterrows():\n",
    "    anime_id = diccionario_animes.get(row['anime_id'], random.randint(1,10000))\n",
    "    if pd.isna(row['Producers']):\n",
    "        continue\n",
    "    producers = [g.strip() for g in row['Producers'].split(',')]\n",
    "    for g in producers:\n",
    "        producer_id = diccionario_producers.get(g, random.randint(1,1549))\n",
    "        text += f\"INSERT INTO animes_producers (id, producer_id, anime_id) VALUES ({id}, {producer_id}, {anime_id});\\n\"\n",
    "        id += 1\n",
    "\n",
    "with open('inserts_animes_producers.sql','w',encoding =\"utf-8\") as archivo:\n",
    "    archivo.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12c9f14",
   "metadata": {},
   "source": [
    "##ANIMES LICENSORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d5f5ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "anime_dataset = pd.read_csv('csv/anime-dataset-2023.csv',sep=',')\n",
    "animes = anime_dataset[['anime_id','Licensors']].head(5000)\n",
    "text = '-- migrate:up \\n\\n'\n",
    "id = 1\n",
    "for index, row in animes.iterrows():\n",
    "    anime_id = diccionario_animes.get(row['anime_id'], random.randint(1,10000))\n",
    "    if pd.isna(row['Licensors']):\n",
    "        continue\n",
    "    licensors = [g.strip() for g in row['Licensors'].split(',')]\n",
    "    for g in licensors:\n",
    "        licensor_id = diccionario_licensors.get(g, random.randint(1,88))\n",
    "        text += f\"INSERT INTO animes_licensors (id, licensor_id, anime_id) VALUES ({id}, {licensor_id}, {anime_id});\\n\"\n",
    "        id += 1\n",
    "\n",
    "with open('inserts_animes_licensors.sql','w',encoding =\"utf-8\") as archivo:\n",
    "    archivo.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607af5e5",
   "metadata": {},
   "source": [
    "##ANIMES STUDIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc8a7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "anime_dataset = pd.read_csv('csv/anime-dataset-2023.csv',sep=',')\n",
    "animes = anime_dataset[['anime_id','Studios']].head(5000)\n",
    "text = '-- migrate:up \\n\\n'\n",
    "id = 1\n",
    "for index, row in animes.iterrows():\n",
    "    anime_id = diccionario_animes.get(row['anime_id'], random.randint(1,10000))\n",
    "    if pd.isna(row['Studios']):\n",
    "        continue\n",
    "    studios = [g.strip() for g in row['Studios'].split(',')]\n",
    "    for g in studios:\n",
    "        studio_id = diccionario_studios.get(g, random.randint(1,1044))\n",
    "        text += f\"INSERT INTO animes_studios (id, studio_id, anime_id) VALUES ({id}, {studio_id}, {anime_id});\\n\"\n",
    "        id += 1\n",
    "\n",
    "with open('inserts_animes_studios.sql','w',encoding =\"utf-8\") as archivo:\n",
    "    archivo.write(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
